{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Done\n",
      "Loading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, csv, itertools, glob\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataloader import train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport configs\n",
    "cfgs = configs\n",
    "cuda = cfgs.USE_CUDA\n",
    "%aimport model\n",
    "CAN = model.CAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(output, target):\n",
    "    # takes in two tensors to compute accuracy\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "#     print(\"Output: \")\n",
    "#     print(output.data.squeeze().cpu().numpy())\n",
    "#     print(\"Pred: \")\n",
    "#     print(pred.squeeze().cpu().numpy())\n",
    "#     print(\"Target: \")\n",
    "#     print(target.data.cpu().numpy())\n",
    "    return correct, target.size(0)\n",
    "\n",
    "def run_trainer(model_path, model, train_loader, test_loader, get_acc, resume, num_epoch):\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    def save_checkpoint(state, is_best, filename=model_path+'checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, model_path+'model_best.pth.tar')\n",
    "    def get_last_checkpoint(model_path):\n",
    "        fs = sorted([f for f in os.listdir(model_path) if 'Epoch' in f], key=lambda k: int(k.split()[1]))\n",
    "        return model_path+fs[-1] if len(fs) > 0 else None\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_res = 0\n",
    "    resume_state = get_last_checkpoint(model_path) if resume else None\n",
    "    if resume_state and os.path.isfile(resume_state):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume_state))\n",
    "        checkpoint = torch.load(resume_state)\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "        best_res = checkpoint['val_acc']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), **cfgs.OPT_PARAM)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume_state, checkpoint['epoch']))\n",
    "    else:\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), **cfgs.OPT_PARAM)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5) # optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5)\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        total, total_correct = 0., 0.\n",
    "        for batch_idx, (img_feats, question, answer) in enumerate(train_loader):\n",
    "            img_feats, question, answer = Variable(img_feats.float()), Variable(question.float()), Variable(answer.long())\n",
    "            if cuda:\n",
    "                img_feats, question, answer = img_feats.cuda(), question.cuda(), answer.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img_feats, question)\n",
    "            loss = criterion(output, answer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            correct, num_instance = get_acc(output, answer)\n",
    "            total_correct += correct\n",
    "            total += num_instance\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Acc: {:.2f}%/{:.2f}%'.format(\n",
    "                    epoch, batch_idx * cfgs.BATCH_SIZE, len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                    100. * correct / num_instance, 100. * total_correct / total ))\n",
    "        \n",
    "        return 100. * total_correct / total\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total, total_correct = 0., 0.\n",
    "        for img_feats, question, answer in test_loader:\n",
    "            img_feats, question, answer = Variable(img_feats.float()), Variable(question.float()), Variable(answer.long())\n",
    "            if cuda:\n",
    "                img_feats, question, answer = img_feats.cuda(), question.cuda(), answer.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img_feats, question)\n",
    "            test_loss += criterion(output, answer).data[0] # sum up batch loss\n",
    "            \n",
    "            correct, num_instance = get_acc(output, answer)\n",
    "            total_correct += correct\n",
    "            total += num_instance\n",
    "\n",
    "        test_acc = 100. * total_correct / total\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, total_correct, total,\n",
    "            test_acc))\n",
    "\n",
    "        return test_acc\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epoch):\n",
    "        is_best = False\n",
    "\n",
    "        train_acc = train(epoch)\n",
    "        val_acc = test()\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_res:\n",
    "            best_res = val_acc\n",
    "            is_best = True\n",
    "\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.cpu().state_dict(),\n",
    "                'train_acc':train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best,\n",
    "            model_path+\"Epoch %d Acc %.4f.pt\"%(epoch, val_acc))\n",
    "\n",
    "        if cuda:\n",
    "            model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/699989 (0%)]\tLoss: 3.321915 Acc: 1.56%/1.56%\n",
      "Train Epoch: 0 [640/699989 (0%)]\tLoss: 2.982973 Acc: 17.19%/20.31%\n",
      "Train Epoch: 0 [1280/699989 (0%)]\tLoss: 2.663429 Acc: 26.56%/20.68%\n",
      "Train Epoch: 0 [1920/699989 (0%)]\tLoss: 2.758787 Acc: 17.19%/20.16%\n",
      "Train Epoch: 0 [2560/699989 (0%)]\tLoss: 2.958366 Acc: 15.62%/20.24%\n",
      "Train Epoch: 0 [3200/699989 (0%)]\tLoss: 2.451110 Acc: 28.12%/20.56%\n",
      "Train Epoch: 0 [3840/699989 (1%)]\tLoss: 2.729137 Acc: 14.06%/20.44%\n",
      "Train Epoch: 0 [4480/699989 (1%)]\tLoss: 2.586968 Acc: 25.00%/20.97%\n",
      "Train Epoch: 0 [5120/699989 (1%)]\tLoss: 2.435516 Acc: 23.44%/21.47%\n",
      "Train Epoch: 0 [5760/699989 (1%)]\tLoss: 2.087361 Acc: 31.25%/22.39%\n",
      "Train Epoch: 0 [6400/699989 (1%)]\tLoss: 1.899316 Acc: 23.44%/22.94%\n",
      "Train Epoch: 0 [7040/699989 (1%)]\tLoss: 1.971354 Acc: 21.88%/23.62%\n",
      "Train Epoch: 0 [7680/699989 (1%)]\tLoss: 1.841437 Acc: 26.56%/23.93%\n",
      "Train Epoch: 0 [8320/699989 (1%)]\tLoss: 2.000602 Acc: 17.19%/24.42%\n",
      "Train Epoch: 0 [8960/699989 (1%)]\tLoss: 1.953207 Acc: 26.56%/24.89%\n",
      "Train Epoch: 0 [9600/699989 (1%)]\tLoss: 1.718696 Acc: 40.62%/25.30%\n",
      "Train Epoch: 0 [10240/699989 (1%)]\tLoss: 1.720724 Acc: 37.50%/25.40%\n",
      "Train Epoch: 0 [10880/699989 (2%)]\tLoss: 1.704972 Acc: 32.81%/25.90%\n",
      "Train Epoch: 0 [11520/699989 (2%)]\tLoss: 2.011285 Acc: 28.12%/26.36%\n",
      "Train Epoch: 0 [12160/699989 (2%)]\tLoss: 1.712009 Acc: 18.75%/26.46%\n",
      "Train Epoch: 0 [12800/699989 (2%)]\tLoss: 1.608393 Acc: 32.81%/26.90%\n",
      "Train Epoch: 0 [13440/699989 (2%)]\tLoss: 1.762989 Acc: 34.38%/27.27%\n",
      "Train Epoch: 0 [14080/699989 (2%)]\tLoss: 1.522604 Acc: 40.62%/27.51%\n",
      "Train Epoch: 0 [14720/699989 (2%)]\tLoss: 1.624153 Acc: 31.25%/27.73%\n",
      "Train Epoch: 0 [15360/699989 (2%)]\tLoss: 1.736429 Acc: 28.12%/28.07%\n",
      "Train Epoch: 0 [16000/699989 (2%)]\tLoss: 1.531798 Acc: 35.94%/28.42%\n",
      "Train Epoch: 0 [16640/699989 (2%)]\tLoss: 1.583839 Acc: 37.50%/28.55%\n",
      "Train Epoch: 0 [17280/699989 (2%)]\tLoss: 1.468125 Acc: 35.94%/28.71%\n",
      "Train Epoch: 0 [17920/699989 (3%)]\tLoss: 1.553164 Acc: 25.00%/28.83%\n",
      "Train Epoch: 0 [18560/699989 (3%)]\tLoss: 1.663178 Acc: 42.19%/29.02%\n",
      "Train Epoch: 0 [19200/699989 (3%)]\tLoss: 1.743803 Acc: 23.44%/29.22%\n",
      "Train Epoch: 0 [19840/699989 (3%)]\tLoss: 1.471142 Acc: 31.25%/29.33%\n",
      "Train Epoch: 0 [20480/699989 (3%)]\tLoss: 1.783350 Acc: 25.00%/29.33%\n",
      "Train Epoch: 0 [21120/699989 (3%)]\tLoss: 1.494897 Acc: 37.50%/29.58%\n",
      "Train Epoch: 0 [21760/699989 (3%)]\tLoss: 1.725989 Acc: 26.56%/29.71%\n",
      "Train Epoch: 0 [22400/699989 (3%)]\tLoss: 1.626387 Acc: 31.25%/29.87%\n",
      "Train Epoch: 0 [23040/699989 (3%)]\tLoss: 1.504019 Acc: 32.81%/30.03%\n",
      "Train Epoch: 0 [23680/699989 (3%)]\tLoss: 1.529754 Acc: 35.94%/30.20%\n",
      "Train Epoch: 0 [24320/699989 (3%)]\tLoss: 1.277780 Acc: 39.06%/30.33%\n",
      "Train Epoch: 0 [24960/699989 (4%)]\tLoss: 1.250243 Acc: 40.62%/30.51%\n",
      "Train Epoch: 0 [25600/699989 (4%)]\tLoss: 1.354948 Acc: 39.06%/30.56%\n",
      "Train Epoch: 0 [26240/699989 (4%)]\tLoss: 1.353000 Acc: 43.75%/30.68%\n",
      "Train Epoch: 0 [26880/699989 (4%)]\tLoss: 1.484692 Acc: 32.81%/30.74%\n",
      "Train Epoch: 0 [27520/699989 (4%)]\tLoss: 1.358740 Acc: 29.69%/30.87%\n",
      "Train Epoch: 0 [28160/699989 (4%)]\tLoss: 1.466722 Acc: 37.50%/30.92%\n",
      "Train Epoch: 0 [28800/699989 (4%)]\tLoss: 1.316002 Acc: 40.62%/31.13%\n",
      "Train Epoch: 0 [29440/699989 (4%)]\tLoss: 1.140775 Acc: 42.19%/31.36%\n",
      "Train Epoch: 0 [30080/699989 (4%)]\tLoss: 1.346734 Acc: 42.19%/31.55%\n",
      "Train Epoch: 0 [30720/699989 (4%)]\tLoss: 1.604085 Acc: 29.69%/31.66%\n",
      "Train Epoch: 0 [31360/699989 (4%)]\tLoss: 1.241160 Acc: 34.38%/31.82%\n",
      "Train Epoch: 0 [32000/699989 (5%)]\tLoss: 1.358023 Acc: 42.19%/31.95%\n",
      "Train Epoch: 0 [32640/699989 (5%)]\tLoss: 1.509341 Acc: 26.56%/32.06%\n",
      "Train Epoch: 0 [33280/699989 (5%)]\tLoss: 1.294340 Acc: 39.06%/32.10%\n",
      "Train Epoch: 0 [33920/699989 (5%)]\tLoss: 1.242537 Acc: 37.50%/32.20%\n",
      "Train Epoch: 0 [34560/699989 (5%)]\tLoss: 1.296854 Acc: 45.31%/32.39%\n",
      "Train Epoch: 0 [35200/699989 (5%)]\tLoss: 1.212166 Acc: 32.81%/32.48%\n",
      "Train Epoch: 0 [35840/699989 (5%)]\tLoss: 1.272187 Acc: 35.94%/32.60%\n",
      "Train Epoch: 0 [36480/699989 (5%)]\tLoss: 1.298796 Acc: 39.06%/32.75%\n",
      "Train Epoch: 0 [37120/699989 (5%)]\tLoss: 1.403933 Acc: 35.94%/32.85%\n",
      "Train Epoch: 0 [37760/699989 (5%)]\tLoss: 1.256031 Acc: 43.75%/32.96%\n",
      "Train Epoch: 0 [38400/699989 (5%)]\tLoss: 1.324389 Acc: 37.50%/33.08%\n",
      "Train Epoch: 0 [39040/699989 (6%)]\tLoss: 1.087465 Acc: 48.44%/33.19%\n",
      "Train Epoch: 0 [39680/699989 (6%)]\tLoss: 1.149548 Acc: 39.06%/33.30%\n",
      "Train Epoch: 0 [40320/699989 (6%)]\tLoss: 1.225791 Acc: 37.50%/33.41%\n",
      "Train Epoch: 0 [40960/699989 (6%)]\tLoss: 1.146632 Acc: 42.19%/33.56%\n",
      "Train Epoch: 0 [41600/699989 (6%)]\tLoss: 1.001507 Acc: 45.31%/33.70%\n",
      "Train Epoch: 0 [42240/699989 (6%)]\tLoss: 1.133662 Acc: 45.31%/33.87%\n",
      "Train Epoch: 0 [42880/699989 (6%)]\tLoss: 1.049290 Acc: 42.19%/34.06%\n",
      "Train Epoch: 0 [43520/699989 (6%)]\tLoss: 1.147955 Acc: 39.06%/34.15%\n",
      "Train Epoch: 0 [44160/699989 (6%)]\tLoss: 1.033084 Acc: 42.19%/34.23%\n",
      "Train Epoch: 0 [44800/699989 (6%)]\tLoss: 1.117896 Acc: 39.06%/34.34%\n",
      "Train Epoch: 0 [45440/699989 (6%)]\tLoss: 1.002483 Acc: 48.44%/34.49%\n",
      "Train Epoch: 0 [46080/699989 (7%)]\tLoss: 1.022954 Acc: 43.75%/34.57%\n",
      "Train Epoch: 0 [46720/699989 (7%)]\tLoss: 1.002456 Acc: 45.31%/34.69%\n",
      "Train Epoch: 0 [47360/699989 (7%)]\tLoss: 1.102552 Acc: 53.12%/34.82%\n",
      "Train Epoch: 0 [48000/699989 (7%)]\tLoss: 1.112655 Acc: 40.62%/34.94%\n",
      "Train Epoch: 0 [48640/699989 (7%)]\tLoss: 0.993735 Acc: 50.00%/35.07%\n",
      "Train Epoch: 0 [49280/699989 (7%)]\tLoss: 1.046639 Acc: 34.38%/35.14%\n",
      "Train Epoch: 0 [49920/699989 (7%)]\tLoss: 1.034636 Acc: 48.44%/35.25%\n",
      "Train Epoch: 0 [50560/699989 (7%)]\tLoss: 0.918103 Acc: 56.25%/35.33%\n",
      "Train Epoch: 0 [51200/699989 (7%)]\tLoss: 1.003537 Acc: 56.25%/35.39%\n",
      "Train Epoch: 0 [51840/699989 (7%)]\tLoss: 1.112860 Acc: 40.62%/35.48%\n",
      "Train Epoch: 0 [52480/699989 (7%)]\tLoss: 0.863828 Acc: 53.12%/35.60%\n",
      "Train Epoch: 0 [53120/699989 (8%)]\tLoss: 0.960457 Acc: 50.00%/35.67%\n",
      "Train Epoch: 0 [53760/699989 (8%)]\tLoss: 0.907618 Acc: 40.62%/35.76%\n",
      "Train Epoch: 0 [54400/699989 (8%)]\tLoss: 1.102856 Acc: 42.19%/35.84%\n",
      "Train Epoch: 0 [55040/699989 (8%)]\tLoss: 1.051988 Acc: 37.50%/35.89%\n",
      "Train Epoch: 0 [55680/699989 (8%)]\tLoss: 1.073240 Acc: 39.06%/35.96%\n",
      "Train Epoch: 0 [56320/699989 (8%)]\tLoss: 1.076452 Acc: 39.06%/36.05%\n",
      "Train Epoch: 0 [56960/699989 (8%)]\tLoss: 1.014244 Acc: 51.56%/36.16%\n",
      "Train Epoch: 0 [57600/699989 (8%)]\tLoss: 1.047815 Acc: 48.44%/36.26%\n",
      "Train Epoch: 0 [58240/699989 (8%)]\tLoss: 1.069528 Acc: 43.75%/36.32%\n",
      "Train Epoch: 0 [58880/699989 (8%)]\tLoss: 1.113285 Acc: 45.31%/36.41%\n",
      "Train Epoch: 0 [59520/699989 (9%)]\tLoss: 1.130644 Acc: 37.50%/36.46%\n",
      "Train Epoch: 0 [60160/699989 (9%)]\tLoss: 1.022516 Acc: 48.44%/36.54%\n",
      "Train Epoch: 0 [60800/699989 (9%)]\tLoss: 1.109380 Acc: 35.94%/36.58%\n",
      "Train Epoch: 0 [61440/699989 (9%)]\tLoss: 1.059880 Acc: 45.31%/36.61%\n",
      "Train Epoch: 0 [62080/699989 (9%)]\tLoss: 0.908890 Acc: 54.69%/36.65%\n",
      "Train Epoch: 0 [62720/699989 (9%)]\tLoss: 1.008507 Acc: 42.19%/36.72%\n",
      "Train Epoch: 0 [63360/699989 (9%)]\tLoss: 1.005822 Acc: 34.38%/36.76%\n",
      "Train Epoch: 0 [64000/699989 (9%)]\tLoss: 0.950010 Acc: 50.00%/36.85%\n",
      "Train Epoch: 0 [64640/699989 (9%)]\tLoss: 1.078709 Acc: 34.38%/36.90%\n",
      "Train Epoch: 0 [65280/699989 (9%)]\tLoss: 0.948292 Acc: 42.19%/36.94%\n",
      "Train Epoch: 0 [65920/699989 (9%)]\tLoss: 1.015385 Acc: 40.62%/36.99%\n",
      "Train Epoch: 0 [66560/699989 (10%)]\tLoss: 1.122039 Acc: 29.69%/37.06%\n",
      "Train Epoch: 0 [67200/699989 (10%)]\tLoss: 1.096899 Acc: 35.94%/37.13%\n",
      "Train Epoch: 0 [67840/699989 (10%)]\tLoss: 1.011045 Acc: 50.00%/37.21%\n",
      "Train Epoch: 0 [68480/699989 (10%)]\tLoss: 0.914684 Acc: 42.19%/37.27%\n",
      "Train Epoch: 0 [69120/699989 (10%)]\tLoss: 0.961813 Acc: 56.25%/37.36%\n",
      "Train Epoch: 0 [69760/699989 (10%)]\tLoss: 1.042152 Acc: 46.88%/37.42%\n",
      "Train Epoch: 0 [70400/699989 (10%)]\tLoss: 0.991335 Acc: 45.31%/37.48%\n",
      "Train Epoch: 0 [71040/699989 (10%)]\tLoss: 0.965460 Acc: 35.94%/37.53%\n",
      "Train Epoch: 0 [71680/699989 (10%)]\tLoss: 1.256253 Acc: 43.75%/37.55%\n",
      "Train Epoch: 0 [72320/699989 (10%)]\tLoss: 1.106931 Acc: 37.50%/37.59%\n",
      "Train Epoch: 0 [72960/699989 (10%)]\tLoss: 0.991286 Acc: 40.62%/37.64%\n",
      "Train Epoch: 0 [73600/699989 (11%)]\tLoss: 1.037694 Acc: 40.62%/37.66%\n",
      "Train Epoch: 0 [74240/699989 (11%)]\tLoss: 0.985820 Acc: 48.44%/37.72%\n",
      "Train Epoch: 0 [74880/699989 (11%)]\tLoss: 1.090243 Acc: 40.62%/37.73%\n",
      "Train Epoch: 0 [75520/699989 (11%)]\tLoss: 0.879318 Acc: 48.44%/37.78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [76160/699989 (11%)]\tLoss: 0.933280 Acc: 42.19%/37.82%\n",
      "Train Epoch: 0 [76800/699989 (11%)]\tLoss: 0.883422 Acc: 45.31%/37.86%\n",
      "Train Epoch: 0 [77440/699989 (11%)]\tLoss: 0.999142 Acc: 37.50%/37.89%\n",
      "Train Epoch: 0 [78080/699989 (11%)]\tLoss: 1.052877 Acc: 39.06%/37.94%\n",
      "Train Epoch: 0 [78720/699989 (11%)]\tLoss: 0.910115 Acc: 37.50%/37.97%\n",
      "Train Epoch: 0 [79360/699989 (11%)]\tLoss: 1.098190 Acc: 40.62%/38.00%\n",
      "Train Epoch: 0 [80000/699989 (11%)]\tLoss: 1.070198 Acc: 42.19%/38.05%\n",
      "Train Epoch: 0 [80640/699989 (12%)]\tLoss: 0.964588 Acc: 54.69%/38.09%\n",
      "Train Epoch: 0 [81280/699989 (12%)]\tLoss: 0.992613 Acc: 53.12%/38.15%\n",
      "Train Epoch: 0 [81920/699989 (12%)]\tLoss: 1.222030 Acc: 39.06%/38.21%\n",
      "Train Epoch: 0 [82560/699989 (12%)]\tLoss: 1.010980 Acc: 53.12%/38.24%\n",
      "Train Epoch: 0 [83200/699989 (12%)]\tLoss: 1.076584 Acc: 35.94%/38.29%\n",
      "Train Epoch: 0 [83840/699989 (12%)]\tLoss: 0.920271 Acc: 50.00%/38.35%\n",
      "Train Epoch: 0 [84480/699989 (12%)]\tLoss: 1.152637 Acc: 32.81%/38.37%\n",
      "Train Epoch: 0 [85120/699989 (12%)]\tLoss: 1.081444 Acc: 45.31%/38.39%\n",
      "Train Epoch: 0 [85760/699989 (12%)]\tLoss: 1.067209 Acc: 39.06%/38.40%\n",
      "Train Epoch: 0 [86400/699989 (12%)]\tLoss: 1.175871 Acc: 35.94%/38.44%\n",
      "Train Epoch: 0 [87040/699989 (12%)]\tLoss: 0.899972 Acc: 51.56%/38.49%\n",
      "Train Epoch: 0 [87680/699989 (13%)]\tLoss: 0.973676 Acc: 43.75%/38.53%\n",
      "Train Epoch: 0 [88320/699989 (13%)]\tLoss: 0.955724 Acc: 45.31%/38.59%\n",
      "Train Epoch: 0 [88960/699989 (13%)]\tLoss: 0.976520 Acc: 39.06%/38.66%\n",
      "Train Epoch: 0 [89600/699989 (13%)]\tLoss: 1.001463 Acc: 35.94%/38.67%\n",
      "Train Epoch: 0 [90240/699989 (13%)]\tLoss: 1.082016 Acc: 51.56%/38.72%\n",
      "Train Epoch: 0 [90880/699989 (13%)]\tLoss: 1.022279 Acc: 48.44%/38.75%\n",
      "Train Epoch: 0 [91520/699989 (13%)]\tLoss: 1.066438 Acc: 34.38%/38.76%\n",
      "Train Epoch: 0 [92160/699989 (13%)]\tLoss: 1.027862 Acc: 46.88%/38.78%\n",
      "Train Epoch: 0 [92800/699989 (13%)]\tLoss: 1.063091 Acc: 37.50%/38.81%\n",
      "Train Epoch: 0 [93440/699989 (13%)]\tLoss: 1.171830 Acc: 35.94%/38.83%\n",
      "Train Epoch: 0 [94080/699989 (13%)]\tLoss: 1.039963 Acc: 43.75%/38.89%\n",
      "Train Epoch: 0 [94720/699989 (14%)]\tLoss: 1.155194 Acc: 39.06%/38.94%\n",
      "Train Epoch: 0 [95360/699989 (14%)]\tLoss: 0.938457 Acc: 42.19%/38.98%\n",
      "Train Epoch: 0 [96000/699989 (14%)]\tLoss: 1.019276 Acc: 39.06%/39.01%\n",
      "Train Epoch: 0 [96640/699989 (14%)]\tLoss: 1.033802 Acc: 39.06%/39.05%\n",
      "Train Epoch: 0 [97280/699989 (14%)]\tLoss: 1.180527 Acc: 42.19%/39.09%\n",
      "Train Epoch: 0 [97920/699989 (14%)]\tLoss: 1.038445 Acc: 32.81%/39.12%\n",
      "Train Epoch: 0 [98560/699989 (14%)]\tLoss: 1.179396 Acc: 31.25%/39.16%\n",
      "Train Epoch: 0 [99200/699989 (14%)]\tLoss: 1.110574 Acc: 40.62%/39.18%\n",
      "Train Epoch: 0 [99840/699989 (14%)]\tLoss: 1.059737 Acc: 42.19%/39.23%\n",
      "Train Epoch: 0 [100480/699989 (14%)]\tLoss: 1.027548 Acc: 48.44%/39.24%\n",
      "Train Epoch: 0 [101120/699989 (14%)]\tLoss: 0.929045 Acc: 50.00%/39.28%\n",
      "Train Epoch: 0 [101760/699989 (15%)]\tLoss: 0.975206 Acc: 46.88%/39.29%\n",
      "Train Epoch: 0 [102400/699989 (15%)]\tLoss: 1.050902 Acc: 37.50%/39.31%\n",
      "Train Epoch: 0 [103040/699989 (15%)]\tLoss: 1.014587 Acc: 32.81%/39.34%\n",
      "Train Epoch: 0 [103680/699989 (15%)]\tLoss: 1.072423 Acc: 48.44%/39.35%\n",
      "Train Epoch: 0 [104320/699989 (15%)]\tLoss: 0.999938 Acc: 39.06%/39.37%\n",
      "Train Epoch: 0 [104960/699989 (15%)]\tLoss: 0.973629 Acc: 46.88%/39.40%\n",
      "Train Epoch: 0 [105600/699989 (15%)]\tLoss: 1.017595 Acc: 50.00%/39.44%\n",
      "Train Epoch: 0 [106240/699989 (15%)]\tLoss: 0.987227 Acc: 40.62%/39.46%\n",
      "Train Epoch: 0 [106880/699989 (15%)]\tLoss: 0.977448 Acc: 45.31%/39.48%\n",
      "Train Epoch: 0 [107520/699989 (15%)]\tLoss: 0.963791 Acc: 45.31%/39.52%\n",
      "Train Epoch: 0 [108160/699989 (15%)]\tLoss: 1.079175 Acc: 43.75%/39.56%\n",
      "Train Epoch: 0 [108800/699989 (16%)]\tLoss: 1.035843 Acc: 39.06%/39.58%\n",
      "Train Epoch: 0 [109440/699989 (16%)]\tLoss: 1.133214 Acc: 34.38%/39.60%\n",
      "Train Epoch: 0 [110080/699989 (16%)]\tLoss: 0.999748 Acc: 48.44%/39.62%\n",
      "Train Epoch: 0 [110720/699989 (16%)]\tLoss: 1.088573 Acc: 46.88%/39.64%\n",
      "Train Epoch: 0 [111360/699989 (16%)]\tLoss: 1.000480 Acc: 48.44%/39.67%\n",
      "Train Epoch: 0 [112000/699989 (16%)]\tLoss: 0.985828 Acc: 51.56%/39.71%\n",
      "Train Epoch: 0 [112640/699989 (16%)]\tLoss: 1.227580 Acc: 43.75%/39.74%\n",
      "Train Epoch: 0 [113280/699989 (16%)]\tLoss: 1.075149 Acc: 35.94%/39.76%\n",
      "Train Epoch: 0 [113920/699989 (16%)]\tLoss: 1.017104 Acc: 45.31%/39.78%\n",
      "Train Epoch: 0 [114560/699989 (16%)]\tLoss: 1.065423 Acc: 42.19%/39.80%\n",
      "Train Epoch: 0 [115200/699989 (16%)]\tLoss: 1.113516 Acc: 39.06%/39.82%\n",
      "Train Epoch: 0 [115840/699989 (17%)]\tLoss: 0.987619 Acc: 42.19%/39.83%\n",
      "Train Epoch: 0 [116480/699989 (17%)]\tLoss: 1.063279 Acc: 32.81%/39.84%\n",
      "Train Epoch: 0 [117120/699989 (17%)]\tLoss: 1.165090 Acc: 51.56%/39.87%\n",
      "Train Epoch: 0 [117760/699989 (17%)]\tLoss: 1.111776 Acc: 46.88%/39.89%\n",
      "Train Epoch: 0 [118400/699989 (17%)]\tLoss: 0.974849 Acc: 50.00%/39.92%\n",
      "Train Epoch: 0 [119040/699989 (17%)]\tLoss: 1.167535 Acc: 45.31%/39.95%\n",
      "Train Epoch: 0 [119680/699989 (17%)]\tLoss: 0.926463 Acc: 40.62%/39.97%\n",
      "Train Epoch: 0 [120320/699989 (17%)]\tLoss: 0.894773 Acc: 43.75%/39.98%\n",
      "Train Epoch: 0 [120960/699989 (17%)]\tLoss: 0.936691 Acc: 51.56%/40.01%\n",
      "Train Epoch: 0 [121600/699989 (17%)]\tLoss: 1.075048 Acc: 40.62%/40.03%\n",
      "Train Epoch: 0 [122240/699989 (17%)]\tLoss: 0.928056 Acc: 42.19%/40.06%\n",
      "Train Epoch: 0 [122880/699989 (18%)]\tLoss: 0.945771 Acc: 50.00%/40.09%\n",
      "Train Epoch: 0 [123520/699989 (18%)]\tLoss: 1.082486 Acc: 48.44%/40.11%\n",
      "Train Epoch: 0 [124160/699989 (18%)]\tLoss: 1.070333 Acc: 45.31%/40.11%\n",
      "Train Epoch: 0 [124800/699989 (18%)]\tLoss: 0.970211 Acc: 43.75%/40.13%\n",
      "Train Epoch: 0 [125440/699989 (18%)]\tLoss: 0.906704 Acc: 48.44%/40.15%\n",
      "Train Epoch: 0 [126080/699989 (18%)]\tLoss: 0.922426 Acc: 60.94%/40.18%\n",
      "Train Epoch: 0 [126720/699989 (18%)]\tLoss: 0.983957 Acc: 46.88%/40.21%\n",
      "Train Epoch: 0 [127360/699989 (18%)]\tLoss: 1.024201 Acc: 43.75%/40.23%\n",
      "Train Epoch: 0 [128000/699989 (18%)]\tLoss: 0.999678 Acc: 53.12%/40.25%\n",
      "Train Epoch: 0 [128640/699989 (18%)]\tLoss: 0.981593 Acc: 46.88%/40.28%\n",
      "Train Epoch: 0 [129280/699989 (18%)]\tLoss: 0.901601 Acc: 50.00%/40.30%\n",
      "Train Epoch: 0 [129920/699989 (19%)]\tLoss: 0.972559 Acc: 48.44%/40.32%\n",
      "Train Epoch: 0 [130560/699989 (19%)]\tLoss: 1.056265 Acc: 45.31%/40.34%\n",
      "Train Epoch: 0 [131200/699989 (19%)]\tLoss: 0.981867 Acc: 50.00%/40.35%\n",
      "Train Epoch: 0 [131840/699989 (19%)]\tLoss: 0.942625 Acc: 53.12%/40.37%\n",
      "Train Epoch: 0 [132480/699989 (19%)]\tLoss: 0.981843 Acc: 43.75%/40.40%\n",
      "Train Epoch: 0 [133120/699989 (19%)]\tLoss: 1.012416 Acc: 28.12%/40.41%\n",
      "Train Epoch: 0 [133760/699989 (19%)]\tLoss: 1.018214 Acc: 45.31%/40.43%\n",
      "Train Epoch: 0 [134400/699989 (19%)]\tLoss: 1.131838 Acc: 34.38%/40.45%\n",
      "Train Epoch: 0 [135040/699989 (19%)]\tLoss: 0.907889 Acc: 50.00%/40.48%\n",
      "Train Epoch: 0 [135680/699989 (19%)]\tLoss: 0.940872 Acc: 50.00%/40.50%\n",
      "Train Epoch: 0 [136320/699989 (19%)]\tLoss: 0.991911 Acc: 45.31%/40.55%\n",
      "Train Epoch: 0 [136960/699989 (20%)]\tLoss: 1.022497 Acc: 43.75%/40.57%\n",
      "Train Epoch: 0 [137600/699989 (20%)]\tLoss: 0.967894 Acc: 46.88%/40.59%\n",
      "Train Epoch: 0 [138240/699989 (20%)]\tLoss: 0.967402 Acc: 54.69%/40.61%\n",
      "Train Epoch: 0 [138880/699989 (20%)]\tLoss: 1.083501 Acc: 37.50%/40.62%\n",
      "Train Epoch: 0 [139520/699989 (20%)]\tLoss: 0.964837 Acc: 50.00%/40.66%\n",
      "Train Epoch: 0 [140160/699989 (20%)]\tLoss: 0.953649 Acc: 35.94%/40.66%\n",
      "Train Epoch: 0 [140800/699989 (20%)]\tLoss: 0.968548 Acc: 39.06%/40.66%\n",
      "Train Epoch: 0 [141440/699989 (20%)]\tLoss: 1.025223 Acc: 39.06%/40.67%\n",
      "Train Epoch: 0 [142080/699989 (20%)]\tLoss: 0.924492 Acc: 54.69%/40.70%\n",
      "Train Epoch: 0 [142720/699989 (20%)]\tLoss: 0.933841 Acc: 45.31%/40.72%\n",
      "Train Epoch: 0 [143360/699989 (20%)]\tLoss: 1.058550 Acc: 45.31%/40.74%\n",
      "Train Epoch: 0 [144000/699989 (21%)]\tLoss: 0.940161 Acc: 56.25%/40.75%\n",
      "Train Epoch: 0 [144640/699989 (21%)]\tLoss: 0.953204 Acc: 51.56%/40.77%\n",
      "Train Epoch: 0 [145280/699989 (21%)]\tLoss: 0.895141 Acc: 53.12%/40.78%\n",
      "Train Epoch: 0 [145920/699989 (21%)]\tLoss: 1.017444 Acc: 53.12%/40.78%\n",
      "Train Epoch: 0 [146560/699989 (21%)]\tLoss: 1.081647 Acc: 42.19%/40.81%\n",
      "Train Epoch: 0 [147200/699989 (21%)]\tLoss: 1.042389 Acc: 37.50%/40.82%\n",
      "Train Epoch: 0 [147840/699989 (21%)]\tLoss: 0.993636 Acc: 32.81%/40.83%\n",
      "Train Epoch: 0 [148480/699989 (21%)]\tLoss: 1.040147 Acc: 43.75%/40.85%\n",
      "Train Epoch: 0 [149120/699989 (21%)]\tLoss: 1.080564 Acc: 48.44%/40.87%\n",
      "Train Epoch: 0 [149760/699989 (21%)]\tLoss: 1.040854 Acc: 48.44%/40.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [150400/699989 (21%)]\tLoss: 0.901690 Acc: 45.31%/40.90%\n",
      "Train Epoch: 0 [151040/699989 (22%)]\tLoss: 0.977090 Acc: 48.44%/40.91%\n",
      "Train Epoch: 0 [151680/699989 (22%)]\tLoss: 1.023801 Acc: 34.38%/40.94%\n",
      "Train Epoch: 0 [152320/699989 (22%)]\tLoss: 1.000446 Acc: 45.31%/40.96%\n",
      "Train Epoch: 0 [152960/699989 (22%)]\tLoss: 0.967268 Acc: 54.69%/40.98%\n",
      "Train Epoch: 0 [153600/699989 (22%)]\tLoss: 0.971132 Acc: 43.75%/41.01%\n",
      "Train Epoch: 0 [154240/699989 (22%)]\tLoss: 0.935738 Acc: 46.88%/41.03%\n",
      "Train Epoch: 0 [154880/699989 (22%)]\tLoss: 0.912891 Acc: 50.00%/41.06%\n",
      "Train Epoch: 0 [155520/699989 (22%)]\tLoss: 0.918632 Acc: 37.50%/41.09%\n",
      "Train Epoch: 0 [156160/699989 (22%)]\tLoss: 0.914777 Acc: 46.88%/41.11%\n",
      "Train Epoch: 0 [156800/699989 (22%)]\tLoss: 1.067858 Acc: 45.31%/41.12%\n",
      "Train Epoch: 0 [157440/699989 (22%)]\tLoss: 1.108058 Acc: 40.62%/41.13%\n",
      "Train Epoch: 0 [158080/699989 (23%)]\tLoss: 1.046415 Acc: 42.19%/41.14%\n",
      "Train Epoch: 0 [158720/699989 (23%)]\tLoss: 0.982188 Acc: 43.75%/41.16%\n",
      "Train Epoch: 0 [159360/699989 (23%)]\tLoss: 0.947810 Acc: 51.56%/41.20%\n",
      "Train Epoch: 0 [160000/699989 (23%)]\tLoss: 1.024724 Acc: 35.94%/41.21%\n",
      "Train Epoch: 0 [160640/699989 (23%)]\tLoss: 1.042623 Acc: 43.75%/41.22%\n",
      "Train Epoch: 0 [161280/699989 (23%)]\tLoss: 0.958482 Acc: 43.75%/41.23%\n",
      "Train Epoch: 0 [161920/699989 (23%)]\tLoss: 0.992594 Acc: 43.75%/41.25%\n",
      "Train Epoch: 0 [162560/699989 (23%)]\tLoss: 1.023682 Acc: 39.06%/41.27%\n",
      "Train Epoch: 0 [163200/699989 (23%)]\tLoss: 0.944738 Acc: 59.38%/41.29%\n",
      "Train Epoch: 0 [163840/699989 (23%)]\tLoss: 1.044583 Acc: 42.19%/41.31%\n",
      "Train Epoch: 0 [164480/699989 (23%)]\tLoss: 0.923679 Acc: 40.62%/41.34%\n",
      "Train Epoch: 0 [165120/699989 (24%)]\tLoss: 1.021752 Acc: 48.44%/41.35%\n",
      "Train Epoch: 0 [165760/699989 (24%)]\tLoss: 1.078394 Acc: 34.38%/41.36%\n",
      "Train Epoch: 0 [166400/699989 (24%)]\tLoss: 0.966374 Acc: 46.88%/41.37%\n",
      "Train Epoch: 0 [167040/699989 (24%)]\tLoss: 0.979814 Acc: 43.75%/41.37%\n",
      "Train Epoch: 0 [167680/699989 (24%)]\tLoss: 0.969424 Acc: 45.31%/41.39%\n",
      "Train Epoch: 0 [168320/699989 (24%)]\tLoss: 1.007543 Acc: 51.56%/41.41%\n",
      "Train Epoch: 0 [168960/699989 (24%)]\tLoss: 0.970663 Acc: 40.62%/41.42%\n",
      "Train Epoch: 0 [169600/699989 (24%)]\tLoss: 1.031586 Acc: 46.88%/41.43%\n",
      "Train Epoch: 0 [170240/699989 (24%)]\tLoss: 0.890094 Acc: 50.00%/41.44%\n",
      "Train Epoch: 0 [170880/699989 (24%)]\tLoss: 0.930115 Acc: 48.44%/41.47%\n",
      "Train Epoch: 0 [171520/699989 (25%)]\tLoss: 0.802591 Acc: 56.25%/41.49%\n",
      "Train Epoch: 0 [172160/699989 (25%)]\tLoss: 0.984096 Acc: 34.38%/41.49%\n",
      "Train Epoch: 0 [172800/699989 (25%)]\tLoss: 1.003689 Acc: 43.75%/41.49%\n",
      "Train Epoch: 0 [173440/699989 (25%)]\tLoss: 1.036524 Acc: 51.56%/41.52%\n",
      "Train Epoch: 0 [174080/699989 (25%)]\tLoss: 0.958151 Acc: 53.12%/41.53%\n",
      "Train Epoch: 0 [174720/699989 (25%)]\tLoss: 0.982124 Acc: 39.06%/41.54%\n",
      "Train Epoch: 0 [175360/699989 (25%)]\tLoss: 0.915829 Acc: 48.44%/41.54%\n",
      "Train Epoch: 0 [176000/699989 (25%)]\tLoss: 0.957665 Acc: 48.44%/41.57%\n",
      "Train Epoch: 0 [176640/699989 (25%)]\tLoss: 1.212176 Acc: 25.00%/41.59%\n",
      "Train Epoch: 0 [177280/699989 (25%)]\tLoss: 1.052667 Acc: 39.06%/41.60%\n",
      "Train Epoch: 0 [177920/699989 (25%)]\tLoss: 1.017678 Acc: 50.00%/41.61%\n",
      "Train Epoch: 0 [178560/699989 (26%)]\tLoss: 0.918454 Acc: 56.25%/41.62%\n",
      "Train Epoch: 0 [179200/699989 (26%)]\tLoss: 1.026597 Acc: 32.81%/41.63%\n",
      "Train Epoch: 0 [179840/699989 (26%)]\tLoss: 0.977455 Acc: 54.69%/41.65%\n",
      "Train Epoch: 0 [180480/699989 (26%)]\tLoss: 0.992897 Acc: 43.75%/41.66%\n",
      "Train Epoch: 0 [181120/699989 (26%)]\tLoss: 0.955649 Acc: 42.19%/41.67%\n",
      "Train Epoch: 0 [181760/699989 (26%)]\tLoss: 0.987974 Acc: 51.56%/41.69%\n",
      "Train Epoch: 0 [182400/699989 (26%)]\tLoss: 0.922060 Acc: 57.81%/41.70%\n",
      "Train Epoch: 0 [183040/699989 (26%)]\tLoss: 1.083629 Acc: 46.88%/41.70%\n",
      "Train Epoch: 0 [183680/699989 (26%)]\tLoss: 1.031643 Acc: 48.44%/41.72%\n",
      "Train Epoch: 0 [184320/699989 (26%)]\tLoss: 0.889618 Acc: 53.12%/41.74%\n",
      "Train Epoch: 0 [184960/699989 (26%)]\tLoss: 0.915655 Acc: 45.31%/41.75%\n",
      "Train Epoch: 0 [185600/699989 (27%)]\tLoss: 0.962681 Acc: 56.25%/41.76%\n",
      "Train Epoch: 0 [186240/699989 (27%)]\tLoss: 0.968719 Acc: 37.50%/41.77%\n",
      "Train Epoch: 0 [186880/699989 (27%)]\tLoss: 0.919186 Acc: 48.44%/41.80%\n",
      "Train Epoch: 0 [187520/699989 (27%)]\tLoss: 1.044309 Acc: 37.50%/41.81%\n",
      "Train Epoch: 0 [188160/699989 (27%)]\tLoss: 1.031395 Acc: 37.50%/41.81%\n",
      "Train Epoch: 0 [188800/699989 (27%)]\tLoss: 1.016946 Acc: 42.19%/41.82%\n",
      "Train Epoch: 0 [189440/699989 (27%)]\tLoss: 0.930637 Acc: 45.31%/41.83%\n",
      "Train Epoch: 0 [190080/699989 (27%)]\tLoss: 0.904103 Acc: 60.94%/41.85%\n",
      "Train Epoch: 0 [190720/699989 (27%)]\tLoss: 0.947176 Acc: 43.75%/41.87%\n",
      "Train Epoch: 0 [191360/699989 (27%)]\tLoss: 0.985943 Acc: 50.00%/41.89%\n",
      "Train Epoch: 0 [192000/699989 (27%)]\tLoss: 0.954137 Acc: 43.75%/41.90%\n",
      "Train Epoch: 0 [192640/699989 (28%)]\tLoss: 0.939308 Acc: 50.00%/41.92%\n",
      "Train Epoch: 0 [193280/699989 (28%)]\tLoss: 1.006917 Acc: 39.06%/41.93%\n",
      "Train Epoch: 0 [193920/699989 (28%)]\tLoss: 0.907667 Acc: 53.12%/41.95%\n",
      "Train Epoch: 0 [194560/699989 (28%)]\tLoss: 0.977533 Acc: 51.56%/41.96%\n",
      "Train Epoch: 0 [195200/699989 (28%)]\tLoss: 1.238913 Acc: 29.69%/41.96%\n",
      "Train Epoch: 0 [195840/699989 (28%)]\tLoss: 0.934239 Acc: 53.12%/41.98%\n",
      "Train Epoch: 0 [196480/699989 (28%)]\tLoss: 1.042381 Acc: 48.44%/42.00%\n",
      "Train Epoch: 0 [197120/699989 (28%)]\tLoss: 0.955472 Acc: 51.56%/42.00%\n",
      "Train Epoch: 0 [197760/699989 (28%)]\tLoss: 0.988280 Acc: 39.06%/42.01%\n",
      "Train Epoch: 0 [198400/699989 (28%)]\tLoss: 0.944546 Acc: 43.75%/42.03%\n",
      "Train Epoch: 0 [199040/699989 (28%)]\tLoss: 1.092785 Acc: 46.88%/42.05%\n",
      "Train Epoch: 0 [199680/699989 (29%)]\tLoss: 1.054928 Acc: 40.62%/42.06%\n",
      "Train Epoch: 0 [200320/699989 (29%)]\tLoss: 0.981686 Acc: 43.75%/42.07%\n",
      "Train Epoch: 0 [200960/699989 (29%)]\tLoss: 1.103231 Acc: 35.94%/42.07%\n",
      "Train Epoch: 0 [201600/699989 (29%)]\tLoss: 0.923046 Acc: 50.00%/42.08%\n",
      "Train Epoch: 0 [202240/699989 (29%)]\tLoss: 1.052736 Acc: 51.56%/42.10%\n",
      "Train Epoch: 0 [202880/699989 (29%)]\tLoss: 0.919325 Acc: 51.56%/42.12%\n"
     ]
    }
   ],
   "source": [
    "model = CAN(**cfgs.NET_PARAM)\n",
    "run_trainer(\n",
    "    model_path = './ckpt/', \n",
    "    model = model, \n",
    "    train_loader = train_loader, \n",
    "    test_loader = val_loader, \n",
    "    get_acc = get_acc, \n",
    "    resume = False, \n",
    "    num_epoch = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
